# Тестовое задание - Сервис работы со скважинами

## Используемые зависимости:
- pydantic - 2.6.2
- uvicorn - 0.27.1
- gunicorn - 21.2.0
- asyncpg - 0.29.0
- numpy - 1.26.4
- matplotlib - 3.8.3

## Зависимости для тестирования

- requests - 2.31.0
- pytest - 8.0.2
- pytest - benchmark==4.0.0

## Как запускать

Приложение состоит из двух контейнеров: 
- fastapi-приложение
- база данных (PostgresSQL)

Запускается командой ```docker compose up -d```

## Запуск тестов

Тесты дожны запускаться вне контейнеров.

То есть помимо создания и запуска контейнеров нужно создать виртуальное
окружение и через него запустить тестирование.

```
python -m venv venv
venv/bin/activate
pip install -r requirements.txt
```

Перед запуском тестов само приложение должно быть запущено через
docker.

Тесты запускаются следующим образом:

```
pytest src/tests
```


## Способ хранениях данных

Было перебрано несколько способов хранения данных в СУБД PostgresQL,
от традиционных с отношениями таблиц и внешних ключей:

```
Table well
{
    pk_id uuid,
    name varchar,
    head point2d
}

Table trajectory
{
    fk_well_id uuid,
    md double,
    x double,
    y double,
    x double
}
```

До нетривиальных, где вся информация о скважине находится в одной
таблице в виде одной записи в готовом виде. То есть траектории
записаны сразу в виде массивов, так как функция ARRAY_AGG является
далеко не бесплатным удовольствием:

```
Table well
{
    pk_id uuid,
    name varchar,
    head point2d
    md double[],
    x double[],
    y double[],
    x double[]
}
```

> Поскольку вопрос о производительности был в большем приоритете,
> я счел необязательным соблюдение нормализации базы данных.

Но даже в таком случае, поиск нужной скважины по его ID проходил
долго из-за медленного последовательного сканирования таблицы.

Ситуацию улучшило разбиение таблицы на partitions, где таблица
разбивалась по его идентификатору. Но при этом требование по
производительности все равно не выполнялось.

Мной было принято решение создавать для каждой скважины отдельную
таблицу с названием `well_{uuid}`, в которой находилась лишь одна
запись о скважине в том же виде, что и во `втором код-блоке`,
благодаря чему я смог достичь большей скорости выполнения запроса,
чем через использование partition.

А для соблюдения уникальности имен ведется дополнительная таблица с
названием well_names. При создании новой записи проводится проверка
на наличие имени создаваемой скважины в этой таблице. Соответственно,
при удалении скважины, её имя оттуда стирается.

В файле `src/resources/sql/init.sql` содержатся функции, через которые
И ТОЛЬКО ЧЕРЕЗ КОТОРЫЕ нужно создавать и удалять информацию о скважинах.
